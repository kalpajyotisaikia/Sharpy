import streamlit as st
import pandas as pd
from utils.code_generator import CodeGenerator
from utils.data_loader import DataLoader
import zipfile
import io
import json
from datetime import datetime

st.set_page_config(page_title="Code Export", page_icon="üíª", layout="wide")

st.title("üíª Code Export & Reproducibility")
st.markdown("Generate reproducible Python and SQL code for your analysis")

# Check if data is loaded
if st.session_state.data is None:
    st.warning("‚ö†Ô∏è No data loaded. Please go to the Data Import page first.")
    st.stop()

df = st.session_state.data
code_gen = CodeGenerator()

# Tabs for different export options
tab1, tab2, tab3, tab4 = st.tabs(["üìù Analysis Notebook", "üîß Custom Code", "üóÑÔ∏è SQL Queries", "üì¶ Export Package"])

# Tab 1: Complete Analysis Notebook
with tab1:
    st.subheader("üìù Complete Analysis Notebook")
    st.markdown("Generate a comprehensive Jupyter notebook with your entire analysis workflow")
    
    # Notebook configuration
    st.write("**Notebook Configuration:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        include_data_loading = st.checkbox("Include Data Loading Code", value=True)
        include_exploration = st.checkbox("Include Data Exploration", value=True)
        include_ai_insights = st.checkbox("Include AI Analysis Results", value=True)
    
    with col2:
        include_visualizations = st.checkbox("Include All Visualizations", value=True)
        include_correlations = st.checkbox("Include Correlation Analysis", value=True)
        include_missing_values = st.checkbox("Include Missing Values Analysis", value=True)
    
    # Additional analysis options
    st.write("**Additional Analyses:**")
    additional_analyses = st.multiselect(
        "Select additional analyses to include:",
        ["Outlier Detection", "Statistical Summary", "Data Profiling", "Feature Engineering Examples"],
        default=["Statistical Summary"]
    )
    
    if st.button("üöÄ Generate Complete Notebook", type="primary", key="generate_notebook"):
        with st.spinner("Generating comprehensive analysis notebook..."):
            try:
                # Prepare data source information
                data_source = st.session_state.get('data_source', {
                    'type': 'unknown',
                    'config': {'file_name': 'data.csv'}
                })
                
                # Prepare analysis results
                analysis_results = st.session_state.get('analysis_results', {
                    'summary': 'No AI analysis available',
                    'key_insights': [],
                    'recommended_analyses': []
                })
                
                # Prepare visualizations
                visualizations = []
                if include_visualizations and st.session_state.get('visualizations'):
                    visualizations = [viz['config'] for viz in st.session_state.visualizations]
                
                # Start building the notebook
                notebook_code = f"""# Comprehensive Data Analysis Notebook
# Generated by DevData Analytics on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

print("Data Analysis Notebook - Generated by DevData Analytics")
print("=" * 60)
"""
                
                # Data Loading Section
                if include_data_loading:
                    notebook_code += "\n\n# " + "="*50
                    notebook_code += "\n# DATA LOADING"
                    notebook_code += "\n# " + "="*50 + "\n"
                    notebook_code += code_gen.generate_data_loading_code(
                        data_source.get('type', 'csv'), 
                        data_source.get('config', {})
                    )
                
                # Data Exploration Section
                if include_exploration:
                    notebook_code += "\n\n# " + "="*50
                    notebook_code += "\n# DATA EXPLORATION"
                    notebook_code += "\n# " + "="*50 + "\n"
                    notebook_code += code_gen.generate_data_exploration_code(df)
                
                # AI Analysis Section
                if include_ai_insights and analysis_results:
                    notebook_code += f"\n\n# " + "="*50
                    notebook_code += "\n# AI ANALYSIS INSIGHTS"
                    notebook_code += "\n# " + "="*50 + "\n"
                    notebook_code += f"""
# AI-Generated Summary
print("AI Analysis Summary:")
print('"""{analysis_results.get('summary', 'No summary available')}"""')

print("\\nKey Insights:")
insights = {analysis_results.get('key_insights', [])}
for i, insight in enumerate(insights, 1):
    print(f"{i}. {insight}")

print("\\nRecommended Analyses:")
recommendations = {analysis_results.get('recommended_analyses', [])}
for rec in recommendations:
    print(f"‚Ä¢ {rec}")
"""
                
                # Additional Analyses
                for analysis in additional_analyses:
                    notebook_code += f"\n\n# " + "="*50
                    notebook_code += f"\n# {analysis.upper()}"
                    notebook_code += "\n# " + "="*50 + "\n"
                    
                    if analysis == "Outlier Detection":
                        notebook_code += code_gen.generate_analysis_code("outlier_detection")
                    elif analysis == "Statistical Summary":
                        notebook_code += """
# Statistical Summary
print("Statistical Summary of Numeric Columns:")
print(df.describe())

print("\\nData Types:")
print(df.dtypes.value_counts())

print("\\nDataset Shape:")
print(f"Rows: {df.shape[0]:,}")
print(f"Columns: {df.shape[1]:,}")
"""
                    elif analysis == "Missing Values":
                        notebook_code += code_gen.generate_analysis_code("missing_values")
                    elif analysis == "Data Profiling":
                        notebook_code += """
# Data Profiling
print("Data Profiling Report:")
print("-" * 40)

for col in df.columns:
    print(f"\\nColumn: {col}")
    print(f"  Type: {df[col].dtype}")
    print(f"  Non-null count: {df[col].count():,}")
    print(f"  Null count: {df[col].isnull().sum():,}")
    print(f"  Unique values: {df[col].nunique():,}")
    
    if df[col].dtype in ['int64', 'float64']:
        print(f"  Min: {df[col].min()}")
        print(f"  Max: {df[col].max()}")
        print(f"  Mean: {df[col].mean():.2f}")
    else:
        print(f"  Most common: {df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'N/A'}")
"""
                
                # Correlation Analysis
                if include_correlations:
                    notebook_code += f"\n\n# " + "="*50
                    notebook_code += "\n# CORRELATION ANALYSIS"
                    notebook_code += "\n# " + "="*50 + "\n"
                    notebook_code += code_gen.generate_analysis_code("correlation_analysis")
                
                # Missing Values Analysis
                if include_missing_values:
                    notebook_code += f"\n\n# " + "="*50
                    notebook_code += "\n# MISSING VALUES ANALYSIS"
                    notebook_code += "\n# " + "="*50 + "\n"
                    notebook_code += code_gen.generate_analysis_code("missing_values")
                
                # Visualizations Section
                if include_visualizations and visualizations:
                    notebook_code += f"\n\n# " + "="*50
                    notebook_code += "\n# VISUALIZATIONS"
                    notebook_code += "\n# " + "="*50 + "\n"
                    
                    for i, viz_config in enumerate(visualizations, 1):
                        notebook_code += f"\n# Visualization {i}: {viz_config.get('title', 'Chart')}\n"
                        notebook_code += code_gen.generate_visualization_code(viz_config)
                        notebook_code += "\n"
                
                # Footer
                notebook_code += f"""

# Analysis Complete
print("\\n" + "="*60)
print("Analysis completed successfully!")
print(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("Tool: DevData Analytics")
print("="*60)
"""
                
                st.success("‚úÖ Notebook generated successfully!")
                
                # Display preview
                st.subheader("üìã Notebook Preview")
                with st.expander("View Generated Code", expanded=False):
                    st.code(notebook_code, language='python')
                
                # Download button
                st.download_button(
                    label="üì• Download Jupyter Notebook (.py)",
                    data=notebook_code,
                    file_name=f"data_analysis_notebook_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py",
                    mime="text/x-python",
                    help="Download as Python file (can be converted to .ipynb)"
                )
                
                # Statistics
                lines_count = len(notebook_code.split('\n'))
                st.info(f"üìä Generated notebook contains {lines_count:,} lines of code")
                
            except Exception as e:
                st.error(f"‚ùå Error generating notebook: {str(e)}")

# Tab 2: Custom Code Generation
with tab2:
    st.subheader("üîß Custom Code Generation")
    st.markdown("Generate specific code snippets for your analysis needs")
    
    # Code type selector
    code_type = st.selectbox(
        "Select code type to generate:",
        [
            "Data Loading Only",
            "Data Exploration",
            "Specific Visualization",
            "Data Cleaning Pipeline",
            "Statistical Analysis",
            "Custom Analysis Function"
        ]
    )
    
    if code_type == "Data Loading Only":
        st.write("**Data Loading Configuration:**")
        
        if st.session_state.get('data_source'):
            source_info = st.session_state.data_source
            st.info(f"Current data source: {source_info.get('type', 'Unknown').title()}")
            
            if st.button("Generate Data Loading Code"):
                code = code_gen.generate_data_loading_code(
                    source_info.get('type', 'csv'),
                    source_info.get('config', {})
                )
                st.code(code, language='python')
                
                st.download_button(
                    label="üì• Download Code",
                    data=code,
                    file_name="data_loading.py",
                    mime="text/x-python"
                )
    
    elif code_type == "Data Exploration":
        if st.button("Generate Data Exploration Code"):
            code = code_gen.generate_data_exploration_code(df)
            st.code(code, language='python')
            
            st.download_button(
                label="üì• Download Code",
                data=code,
                file_name="data_exploration.py",
                mime="text/x-python"
            )
    
    elif code_type == "Specific Visualization":
        st.write("**Select Visualization:**")
        
        if st.session_state.get('visualizations'):
            viz_options = [f"{i+1}. {viz['title']}" for i, viz in enumerate(st.session_state.visualizations)]
            selected_viz_idx = st.selectbox("Choose visualization:", range(len(viz_options)), format_func=lambda x: viz_options[x])
            
            if st.button("Generate Visualization Code"):
                selected_viz = st.session_state.visualizations[selected_viz_idx]
                code = code_gen.generate_visualization_code(selected_viz['config'])
                st.code(code, language='python')
                
                st.download_button(
                    label="üì• Download Code",
                    data=code,
                    file_name=f"visualization_{selected_viz_idx+1}.py",
                    mime="text/x-python"
                )
        else:
            st.info("No visualizations available. Create some visualizations first.")
    
    elif code_type == "Data Cleaning Pipeline":
        st.write("**Data Cleaning Options:**")
        
        col1, col2 = st.columns(2)
        with col1:
            handle_missing = st.checkbox("Handle Missing Values", value=True)
            remove_duplicates = st.checkbox("Remove Duplicates", value=True)
            handle_outliers = st.checkbox("Handle Outliers", value=False)
        
        with col2:
            standardize_columns = st.checkbox("Standardize Column Names", value=True)
            convert_types = st.checkbox("Convert Data Types", value=True)
            validate_data = st.checkbox("Add Data Validation", value=True)
        
        if st.button("Generate Cleaning Pipeline"):
            cleaning_code = """
# Data Cleaning Pipeline
import pandas as pd
import numpy as np
from scipy import stats

def clean_data(df):
    \"\"\"
    Comprehensive data cleaning pipeline
    \"\"\"
    print("Starting data cleaning pipeline...")
    original_shape = df.shape
    
    # Create a copy to avoid modifying original data
    df_cleaned = df.copy()
    
"""
            
            if handle_missing:
                cleaning_code += """
    # Handle Missing Values
    print(f"Missing values before cleaning: {df_cleaned.isnull().sum().sum()}")
    
    # Fill numeric columns with median
    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns
    df_cleaned[numeric_cols] = df_cleaned[numeric_cols].fillna(df_cleaned[numeric_cols].median())
    
    # Fill categorical columns with mode
    categorical_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns
    for col in categorical_cols:
        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0] if len(df_cleaned[col].mode()) > 0 else 'Unknown')
    
    print(f"Missing values after cleaning: {df_cleaned.isnull().sum().sum()}")
"""
            
            if remove_duplicates:
                cleaning_code += """
    # Remove Duplicates
    duplicates_before = df_cleaned.duplicated().sum()
    df_cleaned = df_cleaned.drop_duplicates()
    duplicates_after = df_cleaned.duplicated().sum()
    print(f"Duplicates removed: {duplicates_before - duplicates_after}")
"""
            
            if handle_outliers:
                cleaning_code += """
    # Handle Outliers (IQR method)
    for col in numeric_cols:
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers_count = len(df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)])
        if outliers_count > 0:
            print(f"Outliers in {col}: {outliers_count}")
            # Cap outliers instead of removing them
            df_cleaned[col] = df_cleaned[col].clip(lower_bound, upper_bound)
"""
            
            if standardize_columns:
                cleaning_code += """
    # Standardize Column Names
    df_cleaned.columns = df_cleaned.columns.str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True)
    print("Column names standardized")
"""
            
            if convert_types:
                cleaning_code += """
    # Convert Data Types
    for col in df_cleaned.columns:
        # Try to convert string numbers to numeric
        if df_cleaned[col].dtype == 'object':
            try:
                df_cleaned[col] = pd.to_numeric(df_cleaned[col])
                print(f"Converted {col} to numeric")
            except:
                pass
        
        # Convert datetime strings
        if df_cleaned[col].dtype == 'object' and 'date' in col.lower():
            try:
                df_cleaned[col] = pd.to_datetime(df_cleaned[col])
                print(f"Converted {col} to datetime")
            except:
                pass
"""
            
            if validate_data:
                cleaning_code += """
    # Data Validation
    print("\\nData validation results:")
    print(f"Original shape: {original_shape}")
    print(f"Cleaned shape: {df_cleaned.shape}")
    print(f"Columns: {list(df_cleaned.columns)}")
    print(f"Data types: {df_cleaned.dtypes.value_counts().to_dict()}")
    print(f"Memory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
"""
            
            cleaning_code += """
    print("Data cleaning pipeline completed!")
    return df_cleaned

# Apply cleaning pipeline
df_clean = clean_data(df)
"""
            
            st.code(cleaning_code, language='python')
            
            st.download_button(
                label="üì• Download Cleaning Pipeline",
                data=cleaning_code,
                file_name="data_cleaning_pipeline.py",
                mime="text/x-python"
            )
    
    elif code_type == "Statistical Analysis":
        st.write("**Statistical Analysis Options:**")
        
        analysis_options = st.multiselect(
            "Select analyses to include:",
            ["Descriptive Statistics", "Normality Tests", "Correlation Analysis", "Hypothesis Testing", "ANOVA"],
            default=["Descriptive Statistics", "Correlation Analysis"]
        )
        
        if st.button("Generate Statistical Analysis Code"):
            stats_code = """
# Statistical Analysis
import pandas as pd
import numpy as np
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

"""
            
            if "Descriptive Statistics" in analysis_options:
                stats_code += """
# Descriptive Statistics
print("DESCRIPTIVE STATISTICS")
print("=" * 50)
print(df.describe())

# Additional statistics
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    print(f"\\n{col}:")
    print(f"  Skewness: {df[col].skew():.3f}")
    print(f"  Kurtosis: {df[col].kurtosis():.3f}")
    print(f"  Variance: {df[col].var():.3f}")
"""
            
            if "Normality Tests" in analysis_options:
                stats_code += """
# Normality Tests
print("\\n\\nNORMALITY TESTS")
print("=" * 50)
for col in numeric_cols:
    statistic, p_value = stats.shapiro(df[col].dropna().sample(min(5000, len(df))))
    print(f"{col}: Shapiro-Wilk p-value = {p_value:.6f}")
    if p_value > 0.05:
        print(f"  -> Appears normally distributed")
    else:
        print(f"  -> Not normally distributed")
"""
            
            if "Correlation Analysis" in analysis_options:
                stats_code += """
# Correlation Analysis
print("\\n\\nCORRELATION ANALYSIS")
print("=" * 50)
correlation_matrix = df[numeric_cols].corr()
print("Correlation Matrix:")
print(correlation_matrix)

# Find strong correlations
strong_correlations = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_val = correlation_matrix.iloc[i, j]
        if abs(corr_val) > 0.7:
            print(f"Strong correlation: {correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {corr_val:.3f}")
"""
            
            if "Hypothesis Testing" in analysis_options:
                stats_code += """
# Hypothesis Testing Examples
print("\\n\\nHYPOTHESIS TESTING")
print("=" * 50)

# Example: T-test between groups (if applicable)
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
if len(categorical_cols) > 0 and len(numeric_cols) > 0:
    cat_col = categorical_cols[0]
    num_col = numeric_cols[0]
    
    unique_categories = df[cat_col].unique()[:2]  # Take first 2 categories
    if len(unique_categories) == 2:
        group1 = df[df[cat_col] == unique_categories[0]][num_col].dropna()
        group2 = df[df[cat_col] == unique_categories[1]][num_col].dropna()
        
        t_stat, p_value = stats.ttest_ind(group1, group2)
        print(f"T-test: {unique_categories[0]} vs {unique_categories[1]} for {num_col}")
        print(f"  T-statistic: {t_stat:.3f}")
        print(f"  P-value: {p_value:.6f}")
        print(f"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}")
"""
            
            st.code(stats_code, language='python')
            
            st.download_button(
                label="üì• Download Statistical Analysis",
                data=stats_code,
                file_name="statistical_analysis.py",
                mime="text/x-python"
            )

# Tab 3: SQL Queries
with tab3:
    st.subheader("üóÑÔ∏è SQL Query Generation")
    st.markdown("Generate SQL queries for database analysis and ETL processes")
    
    # Table name configuration
    table_name = st.text_input("Table Name:", value="data_table", help="Name of the table in your database")
    
    # Generate SQL queries
    if st.button("üîç Generate SQL Queries", type="primary"):
        with st.spinner("Generating SQL queries..."):
            try:
                sql_queries = code_gen.generate_sql_queries(df, table_name)
                
                st.success("‚úÖ SQL queries generated successfully!")
                
                # Display queries in tabs
                query_tabs = st.tabs(list(sql_queries.keys()))
                
                for i, (query_name, query_sql) in enumerate(sql_queries.items()):
                    with query_tabs[i]:
                        st.subheader(f"üìù {query_name.replace('_', ' ').title()}")
                        st.code(query_sql, language='sql')
                        
                        st.download_button(
                            label="üì• Download Query",
                            data=query_sql,
                            file_name=f"{query_name}.sql",
                            mime="text/plain",
                            key=f"download_{query_name}"
                        )
                
                # Download all queries as a zip file
                st.markdown("---")
                st.subheader("üì¶ Download All Queries")
                
                if st.button("üì• Download All SQL Queries (ZIP)"):
                    # Create zip file in memory
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for query_name, query_sql in sql_queries.items():
                            zip_file.writestr(f"{query_name}.sql", query_sql)
                        
                        # Add a README
                        readme_content = f"""# SQL Queries for {table_name}

Generated by DevData Analytics on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Files Included:
{chr(10).join([f"- {name}.sql: {name.replace('_', ' ').title()}" for name in sql_queries.keys()])}

## Usage:
1. Replace '{table_name}' with your actual table name if different
2. Modify connection parameters as needed
3. Run queries in your preferred SQL environment

## Dataset Information:
- Rows: {len(df):,}
- Columns: {len(df.columns)}
- Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}
- Categorical columns: {len(df.select_dtypes(include=['object', 'category']).columns)}
"""
                        zip_file.writestr("README.md", readme_content)
                    
                    zip_buffer.seek(0)
                    
                    st.download_button(
                        label="üì• Download ZIP File",
                        data=zip_buffer.getvalue(),
                        file_name=f"sql_queries_{table_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                        mime="application/zip"
                    )
                
            except Exception as e:
                st.error(f"‚ùå Error generating SQL queries: {str(e)}")
    
    # Custom SQL query builder
    st.markdown("---")
    st.subheader("üõ†Ô∏è Custom SQL Query Builder")
    
    query_type = st.selectbox(
        "Select query type:",
        ["SELECT with filters", "GROUP BY analysis", "JOIN template", "Window functions", "Data quality checks"]
    )
    
    if query_type == "SELECT with filters":
        selected_columns = st.multiselect("Select columns:", df.columns.tolist(), default=df.columns.tolist()[:5])
        filter_column = st.selectbox("Filter column (optional):", [''] + df.columns.tolist())
        
        if st.button("Generate SELECT Query"):
            columns_str = ", ".join(selected_columns) if selected_columns else "*"
            query = f"SELECT {columns_str}\nFROM {table_name}"
            
            if filter_column:
                if df[filter_column].dtype in ['object', 'category']:
                    query += f"\nWHERE {filter_column} = 'your_value'"
                else:
                    query += f"\nWHERE {filter_column} > your_threshold"
            
            query += "\nLIMIT 100;"
            
            st.code(query, language='sql')
            
            st.download_button(
                label="üì• Download Query",
                data=query,
                file_name="custom_select_query.sql",
                mime="text/plain"
            )

# Tab 4: Export Package
with tab4:
    st.subheader("üì¶ Complete Analysis Package")
    st.markdown("Export everything as a comprehensive analysis package")
    
    # Package configuration
    st.write("**Package Configuration:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        include_notebook = st.checkbox("Include Analysis Notebook", value=True)
        include_data_sample = st.checkbox("Include Data Sample (first 1000 rows)", value=True)
        include_visualizations = st.checkbox("Include Visualization Code", value=True)
    
    with col2:
        include_sql_queries = st.checkbox("Include SQL Queries", value=True)
        include_requirements = st.checkbox("Include Requirements.txt", value=True)
        include_readme = st.checkbox("Include README Documentation", value=True)
    
    # Data format for export
    data_format = st.selectbox("Data sample format:", ["CSV", "JSON", "Excel"])
    
    if st.button("üì¶ Generate Analysis Package", type="primary"):
        with st.spinner("Creating comprehensive analysis package..."):
            try:
                # Create zip file in memory
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    
                    # Include data sample
                    if include_data_sample:
                        sample_df = df.head(1000)
                        
                        if data_format == "CSV":
                            csv_buffer = io.StringIO()
                            sample_df.to_csv(csv_buffer, index=False)
                            zip_file.writestr("data_sample.csv", csv_buffer.getvalue())
                        
                        elif data_format == "JSON":
                            json_data = sample_df.to_json(orient='records', indent=2)
                            zip_file.writestr("data_sample.json", json_data)
                        
                        elif data_format == "Excel":
                            excel_buffer = io.BytesIO()
                            sample_df.to_excel(excel_buffer, index=False, engine='openpyxl')
                            zip_file.writestr("data_sample.xlsx", excel_buffer.getvalue())
                    
                    # Include analysis notebook
                    if include_notebook:
                        data_source = st.session_state.get('data_source', {'type': 'csv', 'config': {}})
                        analysis_results = st.session_state.get('analysis_results', {})
                        visualizations = [viz['config'] for viz in st.session_state.get('visualizations', [])]
                        
                        notebook_code = code_gen.generate_complete_notebook(data_source, analysis_results, visualizations)
                        zip_file.writestr("analysis_notebook.py", notebook_code)
                    
                    # Include visualization code
                    if include_visualizations and st.session_state.get('visualizations'):
                        for i, viz in enumerate(st.session_state.visualizations):
                            viz_code = code_gen.generate_visualization_code(viz['config'])
                            zip_file.writestr(f"visualizations/chart_{i+1}_{viz['config']['chart_type']}.py", viz_code)
                    
                    # Include SQL queries
                    if include_sql_queries:
                        sql_queries = code_gen.generate_sql_queries(df, "data_table")
                        for query_name, query_sql in sql_queries.items():
                            zip_file.writestr(f"sql_queries/{query_name}.sql", query_sql)
                    
                    # Include requirements.txt
                    if include_requirements:
                        requirements = """# Data Analysis Requirements
pandas>=1.5.0
numpy>=1.21.0
plotly>=5.10.0
seaborn>=0.11.0
matplotlib>=3.5.0
scipy>=1.9.0
scikit-learn>=1.1.0
jupyter>=1.0.0
openpyxl>=3.0.0
requests>=2.28.0
sqlalchemy>=1.4.0

# Optional: for advanced analysis
statsmodels>=0.13.0
xgboost>=1.6.0
"""
                        zip_file.writestr("requirements.txt", requirements)
                    
                    # Include README
                    if include_readme:
                        readme_content = f"""# Data Analysis Package

Generated by DevData Analytics on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Package Contents

### Data Files
{"- `data_sample." + data_format.lower() + "`: Sample of your dataset (first 1000 rows)" if include_data_sample else ""}

### Analysis Code
{"- `analysis_notebook.py`: Complete analysis notebook with all insights" if include_notebook else ""}
{"- `visualizations/`: Individual visualization scripts" if include_visualizations else ""}
{"- `sql_queries/`: Database queries for analysis" if include_sql_queries else ""}

### Setup Files
{"- `requirements.txt`: Python package dependencies" if include_requirements else ""}

## Getting Started

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run Analysis Notebook**
   ```bash
   python analysis_notebook.py
   ```

3. **Convert to Jupyter Notebook** (optional)
   ```bash
   jupytext --to notebook analysis_notebook.py
   ```

## Dataset Information

- **Rows:** {len(df):,}
- **Columns:** {len(df.columns)}
- **Numeric Columns:** {len(df.select_dtypes(include=[np.number]).columns)}
- **Categorical Columns:** {len(df.select_dtypes(include=['object', 'category']).columns)}
- **Memory Usage:** {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

## Analysis Summary

{st.session_state.get('analysis_results', {}).get('summary', 'No AI analysis summary available')}

## Key Insights

{chr(10).join([f"- {insight}" for insight in st.session_state.get('analysis_results', {}).get('key_insights', [])])}

## Visualizations Created

{chr(10).join([f"- {viz['title']} ({viz['config']['chart_type'].title()})" for viz in st.session_state.get('visualizations', [])])}

## Support

This package was generated by DevData Analytics. For questions or support:
- Review the analysis notebook for detailed explanations
- Check individual visualization scripts for chart-specific code
- Use SQL queries for database-based analysis

---
**Generated by DevData Analytics** - AI-powered data analysis tool
"""
                        zip_file.writestr("README.md", readme_content)
                    
                    # Add project metadata
                    metadata = {
                        "generated_by": "DevData Analytics",
                        "generated_at": datetime.now().isoformat(),
                        "dataset_info": {
                            "rows": len(df),
                            "columns": len(df.columns),
                            "column_names": df.columns.tolist(),
                            "data_types": df.dtypes.astype(str).to_dict(),
                            "memory_usage_mb": round(df.memory_usage(deep=True).sum() / 1024**2, 2)
                        },
                        "analysis_info": {
                            "ai_analysis_available": bool(st.session_state.get('analysis_results')),
                            "visualizations_count": len(st.session_state.get('visualizations', [])),
                            "data_source": st.session_state.get('data_source', {})
                        }
                    }
                    zip_file.writestr("metadata.json", json.dumps(metadata, indent=2))
                
                zip_buffer.seek(0)
                
                st.success("‚úÖ Analysis package created successfully!")
                
                # Package statistics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Package Size", f"{len(zip_buffer.getvalue()) / 1024:.1f} KB")
                with col2:
                    file_count = sum([include_data_sample, include_notebook, include_requirements, include_readme])
                    if include_visualizations:
                        file_count += len(st.session_state.get('visualizations', []))
                    if include_sql_queries:
                        file_count += len(code_gen.generate_sql_queries(df, "data_table"))
                    st.metric("Files Included", file_count)
                with col3:
                    st.metric("Dataset Rows", f"{len(df):,}")
                
                # Download button
                st.download_button(
                    label="üì• Download Complete Package",
                    data=zip_buffer.getvalue(),
                    file_name=f"data_analysis_package_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                    mime="application/zip",
                    help="Download the complete analysis package as a ZIP file"
                )
                
            except Exception as e:
                st.error(f"‚ùå Error creating analysis package: {str(e)}")

# Sidebar: Export Summary
with st.sidebar:
    st.header("üíª Export Summary")
    
    # Current data info
    st.metric("Dataset Rows", f"{len(df):,}")
    st.metric("Columns", len(df.columns))
    
    # Available exports
    st.write("**Available Exports:**")
    st.write("‚Ä¢ Complete analysis notebook")
    st.write("‚Ä¢ Individual code snippets")
    st.write("‚Ä¢ SQL queries")
    st.write("‚Ä¢ Comprehensive package")
    
    # Analysis status
    if st.session_state.get('analysis_results'):
        st.success("‚úÖ AI Analysis Available")
    else:
        st.info("‚ÑπÔ∏è Run AI analysis first")
    
    if st.session_state.get('visualizations'):
        st.success(f"‚úÖ {len(st.session_state.visualizations)} Visualizations")
    else:
        st.info("‚ÑπÔ∏è Create visualizations first")
    
    st.markdown("---")
    st.write("üí° **Tips:**")
    st.write("‚Ä¢ Start with complete notebook for full analysis")
    st.write("‚Ä¢ Use custom code for specific needs")
    st.write("‚Ä¢ SQL queries work with any database")
    st.write("‚Ä¢ Package export includes everything")
